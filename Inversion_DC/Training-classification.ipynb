{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of Classification Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained a classification neural network that is able to distinguish velocity structures with 2 - 7 layers.  \n",
    "Use Love and Rayleigh dispersion curves together with their uncertainty vectors as input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pickle import dump\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "import keras\n",
    "import tensorflow.keras.utils\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up of neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### X_train - training data corresponding to network inputs eg. dispersion curve data\n",
    "### ytrain - training data corresponding to network outputs eg. 1D velocity model\n",
    "\n",
    "x_size=100 # size of input data, dispersion curve sampled logarithmically with 100 points from 1-20 Hz\n",
    "y_size = 6 # size of output data, here 6 different layer numbers (structures with 2 - 7 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Network configuration ###\n",
    "\n",
    "inp1 = Input((x_size,))  #input love dispersion curve\n",
    "hidden_1 = Dense(418,activation=k.relu)(inp1)\n",
    "hidden_1_2 = Dense(716,activation=k.relu)(hidden_1)\n",
    "drop_1 = Dropout(0.05)(hidden_1_2)\n",
    "\n",
    "inp2=Input((x_size,))  #input love uncertainty vector\n",
    "hidden_2=Dense(369,activation=k.relu)(inp2)\n",
    "hidden_2_2=Dense(769,activation=k.relu)(hidden_2)\n",
    "drop_2=Dropout(0.48)(hidden_2_2)\n",
    "\n",
    "inp3 = Input((x_size,))  #input dispersion curve Rayleigh\n",
    "hidden_3 = Dense(117,activation=k.relu)(inp3)\n",
    "hidden_3_2 = Dense(396,activation=k.relu)(hidden_3)\n",
    "drop_3 = Dropout(0.76)(hidden_3_2)\n",
    "\n",
    "inp4=Input((x_size,))  #input uncertainty vector Rayleigh\n",
    "hidden_4=Dense(216,activation=k.relu)(inp4)\n",
    "hidden_4_2=Dense(828,activation=k.relu)(hidden_4)\n",
    "drop_4=Dropout(0.69)(hidden_4_2)\n",
    "\n",
    "merged1=tf.keras.layers.concatenate([drop_1,drop_2]) #merge layers\n",
    "hidden_5=Dense(879,activation=k.relu)(merged1)\n",
    "drop_5 = Dropout(0.78)(hidden_5)\n",
    "\n",
    "merged2=tf.keras.layers.concatenate([drop_3,drop_4])\n",
    "hidden_6=Dense(832,activation=k.relu)(merged2)\n",
    "drop_6 = Dropout(0.04)(hidden_6)\n",
    "\n",
    "merged3=tf.keras.layers.concatenate([drop_5,drop_6])\n",
    "hidden_7=Dense(332,activation=k.relu)(merged3)\n",
    "hidden_7_2=Dense(845,activation=k.relu)(hidden_7)\n",
    "drop_7 = Dropout(0.99)(hidden_7_2)\n",
    "\n",
    "\n",
    "output = Dense(y_size,activation='softmax')(drop_7)\n",
    "\n",
    "model = Model(inputs=[inp1,inp2,inp3,inp4], outputs=output) \n",
    "\n",
    "### Set optimizer ###\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001,decay=0.0, amsgrad=True)\n",
    "\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "### Compile model ###\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer=adam,metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot network\n",
    "tf.keras.utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define callbacks\n",
    "NAN = tensorflow.keras.callbacks.TerminateOnNaN() # If the training loss produces Nans, stop the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data should consist of a mix of dispersion curves forward modelled from structures with different layer numbers (here 2 - 7 layer structures).  \n",
    "Dispersion curves and uncertainty vectors should be logarithmically resampled with 100 samples between 1 - 20 Hz corresponding to the frequency:\n",
    "#### freq=np.logspace(0,1.3,100,base=10)   \n",
    "The phase velocity and uncertainty should be in km/s !!     \n",
    "Layer number is an intiger value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from .npy array; insert your data here\n",
    "input_disp_lov=np.load(\"love_disp.npy\") #input Love Dispersion\n",
    "input_disp_ray=np.load(\"ray_disp.npy\") #input Rayleigh Dispersion\n",
    "input_un_lov=np.load(\"love_un.npy\") #input Love Uncertainty\n",
    "input_un_ray=np.load(\"ray_un.npy\") #input Rayleigh Uncertainty\n",
    "output_num=np.load(\"layer_number.npy\") #output layer number\n",
    "\n",
    "print(input_disp_lov.shape) #shape should be: (number_training_models, 100)\n",
    "print(input_disp_ray.shape)\n",
    "print(input_un_lov.shape)\n",
    "print(input_un_ray.shape)\n",
    "print(output_num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rescale layer number between 0 and 5 (before it was 2 to 7)\n",
    "new_output_num=[]\n",
    "for n in range(len(output_num)):\n",
    "    val=output_num[n]-2\n",
    "    new_output_num.append(val)\n",
    "    \n",
    "print(np.shape(new_output_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram of layer numbers\n",
    "plt.hist(new_output_num,bins=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "#scale data \n",
    "\n",
    "scaler1 = StandardScaler() #for Love dispersion\n",
    "scaler2 = StandardScaler() #for Love uncertainty\n",
    "scaler3 = StandardScaler() #for Rayleigh dispersion\n",
    "scaler4 = StandardScaler() #for Rayleigh uncertainty\n",
    "\n",
    "input_disp_lov_scal = scaler1.fit_transform(input_disp_lov)\n",
    "input_un_lov_scal = scaler2.fit_transform(input_un_lov)\n",
    "input_disp_ray_scal = scaler3.fit_transform(input_disp_ray)\n",
    "input_un_ray_scal = scaler4.fit_transform(input_un_ray)\n",
    "\n",
    "#output doesnÂ´t have to be scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the scaler\n",
    "dump(scaler1, open('./scaler1.pkl', 'wb'))\n",
    "dump(scaler2, open('./scaler2.pkl', 'wb'))\n",
    "dump(scaler3, open('./scaler3.pkl', 'wb'))\n",
    "dump(scaler4, open('./scaler4.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split into training and test set\n",
    "\n",
    "training_disp_lov=input_disp_lov_scal[:560000] #range depends on number of training data\n",
    "testing_disp_lov=input_disp_lov_scal[560000:]\n",
    "\n",
    "training_un_lov=input_un_lov_scal[:560000]\n",
    "testing_un_lov=input_un_lov_scal[560000:]\n",
    "\n",
    "training_disp_ray=input_disp_ray_scal[:560000]\n",
    "testing_disp_ray=input_disp_ray_scal[560000:]\n",
    "\n",
    "training_un_ray=input_un_ray_scal[:560000]\n",
    "testing_un_ray=input_un_ray_scal[560000:]\n",
    "\n",
    "training_num=new_output_num[:560000]\n",
    "testing_num=new_output_num[560000:]\n",
    "\n",
    "print(np.shape(training_disp_lov))\n",
    "print(np.shape(testing_disp_lov))\n",
    "print(np.shape(training_un_lov))\n",
    "print(np.shape(testing_un_lov))\n",
    "print(np.shape(training_disp_ray))\n",
    "print(np.shape(testing_disp_ray))\n",
    "print(np.shape(training_un_ray))\n",
    "print(np.shape(testing_un_ray))\n",
    "print(np.shape(training_num))\n",
    "print(np.shape(testing_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model  \n",
    "Number of epochs and batch size can be adapted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train model\n",
    "results=model.fit([training_disp_lov,training_un_lov,training_disp_ray,training_un_ray], training_num,batch_size=128, epochs=100,verbose=1, shuffle=True,validation_split=0.1,callbacks=[NAN])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save('./model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model\n",
    "loss_and_metrics = model.evaluate([testing_disp_lov,testing_un_lov,testing_disp_ray,testing_un_ray], testing_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Curves\n",
    "fig=plt.figure(figsize=[8,6])\n",
    "\n",
    "ax=fig.add_subplot(111)\n",
    "ax.plot(results.history['loss'],'r',linewidth=3.0)\n",
    "ax.plot(results.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance  \n",
    "Use previously unseen input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test trained model on unseen data; insert your data here\n",
    "unseen_disp_lov=np.load(\"love_disp_unseen.npy\")\n",
    "unseen_un_lov=np.load(\"love_un_unseen.npy\")\n",
    "unseen_disp_ray=np.load(\"ray_disp_unseen.npy\")\n",
    "unseen_un_ray=np.load(\"ray_un_unseen.npy\")\n",
    "unseen_num=np.load(\"layer_num_unseen.npy\")\n",
    "\n",
    "print(np.shape(unseen_disp_lov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data\n",
    "unseen_disp_lov_scal=scaler1.transform(unseen_disp_lov)\n",
    "unseen_un_lov_scal=scaler2.transform(unseen_un_lov)\n",
    "unseen_disp_ray_scal=scaler3.transform(unseen_disp_ray)\n",
    "unseen_un_ray_scal=scaler4.transform(unseen_un_ray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict velocities\n",
    "predictions = model.predict([unseen_disp_lov_scal,unseen_un_lov_scal,unseen_disp_ray_scal,unseen_un_ray_scal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((predictions.shape)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot output for one structure\n",
    "x=np.linspace(2,7,6) #range of layer numbers\n",
    "plt.plot(x,predictions[0])\n",
    "plt.xlabel(\"Number of Layers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted layer number for one structure:\n",
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute error of prediction\n",
    "num_error=[]\n",
    "for n in range(len(predictions)):\n",
    "    index_max = np.argmax(predictions[n])\n",
    "    err=index_max-unseen_num[n]\n",
    "    num_error.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot errors \n",
    "fig=plt.figure(figsize=(6,6))\n",
    "\n",
    "ax=fig.add_subplot(1,1,1)\n",
    "ax.hist(num_error, bins=100)\n",
    "plt.ylabel(\"Number of models\")\n",
    "plt.xlabel(\"Number of layers\")\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot pearson correlation to visualize error\n",
    "\n",
    "line=np.linspace(0,8,50)\n",
    "\n",
    "fig=plt.figure(figsize=(5,5))\n",
    "\n",
    "ax=fig.add_subplot(1,1,1)\n",
    "x=[]\n",
    "for i in range(len(predictions)):\n",
    "    index_max = np.argmax(predictions[i])+2 #+2 to scale data back to original range of 2 - 7 layers\n",
    "    x.append(index_max)\n",
    "\n",
    "y=[]\n",
    "for i in range(len(unseen_num)):\n",
    "    y.append(unseen_num[i])\n",
    "\n",
    "        \n",
    "corr =stats.pearsonr(x, y)\n",
    "Z, xax,yax=np.histogram2d(x,y,bins=50,range=[[0,9],[0,9]])\n",
    "    \n",
    "ax.pcolormesh(xax, yax, Z.T, cmap=\"Greys\")\n",
    "plt.plot(line,line, color=\"red\")\n",
    "plt.xlabel(\"Predicted num layers\",fontsize=12)\n",
    "plt.ylabel(\"True num layers\",fontsize=12)\n",
    "plt.title(\"corr {}\".format(\"%0.3f\" % (corr[0])))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
