{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of Mixture Density Network\n",
    "\n",
    "We trained a mixture density network that returns a complete probability distribution of the S-wave velocity in the upper 100m.  \n",
    "Here we trained it for structures with 3 layers.\n",
    "Use Love and Rayleigh dispersion curves together with their uncertainty vectors as input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pickle import dump\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "import keras\n",
    "import tensorflow.keras.utils\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up of neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function provided by Earp et al. (2020)\n",
    "def log_sum_exp(x, axis=None):\n",
    "    \"\"\"Log-sum-exp trick implementation\"\"\"\n",
    "    x_max = k.max(x, axis=axis, keepdims=True)\n",
    "    return k.log(k.sum(k.exp(x - x_max), axis=axis, keepdims=True))+x_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function provided by Earp et al. (2020)\n",
    "### This is the key function that adjusts the network to produce an MDN\n",
    "### It is a custom loss function\n",
    "### Simple to translate into anything Python ML library you desire\n",
    "\n",
    "def mean_log_Gaussian_like(y_true, parameters):\n",
    "    \"\"\"Mean Log Gaussian Likelihood distribution\n",
    "    Note: The 'c' variable is obtained as global variable\n",
    "    \"\"\"\n",
    "    components = k.reshape(parameters,[-1, c + 2, m])\n",
    "    print(\"parameters are:\",parameters)\n",
    "    print(\"shape of parameters\",parameters.shape)\n",
    "    mu = components[:, :c, :]\n",
    "    sigma = components[:, c, :]\n",
    "    alpha = components[:, c + 1, :]\n",
    "\n",
    "    alpha = k.clip(alpha,1e-8,1.)\n",
    "    sigma = k.clip(sigma,1e-7,25.)\n",
    "    exponent = k.log(alpha) - .5 * float(c) * k.log(2 * np.pi) \\\n",
    "    - float(c) * k.log(sigma) \\\n",
    "    - k.sum((k.expand_dims(y_true,2) - mu)**2, axis=1)/(2*(sigma)**2)\n",
    "\n",
    "    log_gauss = log_sum_exp(exponent, axis=1)\n",
    "    res = - k.mean(log_gauss)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### X_train - training data corresponding to network inputs eg. dispersion curve data\n",
    "### ytrain - training data corresponding to network outputs eg. 1D velocity model\n",
    "\n",
    "x_size=100 # size of input data, dispersion curve sampled logarithmically with 100 points from 1-20 Hz\n",
    "y_size = 3 # size of output data, 3 layers\n",
    "c = y_size # this is used above in the function mean_log_Gaussian_like\n",
    "m = 10 # number of mixtures in Gaussian, should be chosen according to the complexity of the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Network configuration ###\n",
    "\n",
    "inp1 = Input((x_size,))  #input love dispersion curve\n",
    "hidden_1 = Dense(271,activation=k.relu)(inp1)\n",
    "drop_1 = Dropout(0.6)(hidden_1)\n",
    "\n",
    "inp2=Input((x_size,))  #input love uncertainty vector\n",
    "hidden_2=Dense(696,activation=k.relu)(inp2)\n",
    "drop_2=Dropout(0.9)(hidden_2)\n",
    "\n",
    "inp3 = Input((x_size,))  #input dispersion curve Rayleigh\n",
    "hidden_3 = Dense(565,activation=k.relu)(inp3)\n",
    "drop_3 = Dropout(0.1)(hidden_3)\n",
    "\n",
    "inp4=Input((x_size,))  #input uncertainty vector Rayleigh\n",
    "hidden_4=Dense(622,activation=k.relu)(inp4)\n",
    "drop_4=Dropout(0.7)(hidden_4)\n",
    "\n",
    "merged1=tf.keras.layers.concatenate([drop_1,drop_2]) #merge layers\n",
    "hidden_5=Dense(254,activation=k.relu)(merged1)\n",
    "drop_5 = Dropout(0.7)(hidden_5)\n",
    "\n",
    "merged2=tf.keras.layers.concatenate([drop_3,drop_4])\n",
    "hidden_6=Dense(479,activation=k.relu)(merged2)\n",
    "drop_6 = Dropout(0.1)(hidden_6)\n",
    "\n",
    "merged3=tf.keras.layers.concatenate([drop_5,drop_6])\n",
    "hidden_7=Dense(814,activation=k.relu)(merged3)\n",
    "drop_7 = Dropout(0.2)(hidden_7)\n",
    "\n",
    "depth_lay=Dense(281,activation=k.relu)(drop_7)\n",
    "drop_depth_lay = Dropout(0.2)(depth_lay)\n",
    "\n",
    "#MDN layer\n",
    "FC_mus   = Dense(y_size*m)(drop_7)  #mean\n",
    "FC_sigma = Dense(m)(drop_7)  #stdev\n",
    "FC_alpha = Dense(m,activation='softmax')(drop_7)  #weighting parameter\n",
    "\n",
    "output  = tf.keras.layers.concatenate([FC_mus, FC_sigma, FC_alpha], axis=1) #output of MDN for probability of S-wave velocity\n",
    "output2 = Dense(y_size,activation=k.relu)(drop_depth_lay) #output of layer depth\n",
    "\n",
    "model = Model(inputs=[inp1,inp2,inp3,inp4], outputs=[output,output2]) \n",
    "\n",
    "### Set optimizer ###\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001,decay=0.0, amsgrad=True)\n",
    "\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "### Compile model ###\n",
    "model.compile(loss=[mean_log_Gaussian_like,mse],optimizer=adam,metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define callbacks\n",
    "NAN = tensorflow.keras.callbacks.TerminateOnNaN() # If the training loss produces Nans, stop the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dispersion curves and uncertainty vectors should be logarithmically resampled with 100 samples between 1 - 20 Hz corresponding to the frequency:\n",
    "#### freq=np.logspace(0,1.3,100,base=10)\n",
    "\n",
    "The phase velocity and uncertainty should be in km/s !! Layer depth is an intiger value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from .npy array; insert your data here\n",
    "input_disp_lov=np.load(\"love_disp.npy\") #input Love Dispersion\n",
    "input_disp_ray=np.load(\"ray_disp.npy\") #input Rayleigh Dispersion\n",
    "input_un_lov=np.load(\"love_un.npy\") #input Love Uncertainty\n",
    "input_un_ray=np.load(\"ray_un.npy\") #input Rayleigh Uncertainty\n",
    "output_svel=np.load(\"svel.npy\") #output S-wave velocity\n",
    "output_depth=np.load(\"depth.npy\") #output depth of layer boundaries\n",
    "\n",
    "print(input_disp_lov.shape) #shape should be: (number_training_models, 100)\n",
    "print(input_disp_ray.shape)\n",
    "print(input_un_lov.shape)\n",
    "print(input_un_ray.shape)\n",
    "print(output_svel.shape)\n",
    "print(output_depth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print example dispersion curves with uncertainties\n",
    "\n",
    "freq=np.logspace(0,1.3,100,base=10)\n",
    "lov=input_disp_lov[0]\n",
    "lov_un=input_un_lov[0]\n",
    "ray=input_disp_ray[0]\n",
    "ray_un=input_un_ray[0]\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(10,7))\n",
    "ax1=fig.add_subplot(111)\n",
    "\n",
    "ax1.errorbar(freq,lov,yerr=lov_un, label=\"Love\")\n",
    "ax1.errorbar(freq,ray,yerr=ray_un, label=\"Rayleigh\")\n",
    "plt.xlabel(\"Frequency (Hz)\",fontsize=12)\n",
    "plt.ylabel(\"Velocity (m/s)\",fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "#scale data \n",
    "\n",
    "scaler1 = StandardScaler() #for Love dispersion\n",
    "scaler2 = StandardScaler() #for Love uncertainty\n",
    "scaler3 = StandardScaler() #for Rayleigh dispersion\n",
    "scaler4 = StandardScaler() #for Rayleigh uncertainty\n",
    "\n",
    "input_disp_lov_scal = scaler1.fit_transform(input_disp_lov)\n",
    "input_un_lov_scal = scaler2.fit_transform(input_un_lov)\n",
    "input_disp_ray_scal = scaler3.fit_transform(input_disp_ray)\n",
    "input_un_ray_scal = scaler4.fit_transform(input_un_ray)\n",
    "\n",
    "#output doesnÂ´t have to be scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the scaler\n",
    "dump(scaler1, open('./scaler1.pkl', 'wb'))\n",
    "dump(scaler2, open('./scaler2.pkl', 'wb'))\n",
    "dump(scaler3, open('./scaler3.pkl', 'wb'))\n",
    "dump(scaler4, open('./scaler4.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split into training and test set\n",
    "\n",
    "training_disp_lov=input_disp_lov_scal[:130000] #range depends on number of training data\n",
    "testing_disp_lov=input_disp_lov_scal[130000:]\n",
    "\n",
    "training_un_lov=input_un_lov_scal[:130000]\n",
    "testing_un_lov=input_un_lov_scal[130000:]\n",
    "\n",
    "training_disp_ray=input_disp_ray_scal[:130000]\n",
    "testing_disp_ray=input_disp_ray_scal[130000:]\n",
    "\n",
    "training_un_ray=input_un_ray_scal[:130000]\n",
    "testing_un_ray=input_un_ray_scal[130000:]\n",
    "\n",
    "training_svel=output_svel[:130000]\n",
    "testing_svel=output_svel[130000:]\n",
    "\n",
    "training_depth=output_depth[:130000]\n",
    "testing_depth=output_depth[130000:]\n",
    "\n",
    "\n",
    "print(np.shape(training_disp_lov))\n",
    "print(np.shape(testing_disp_lov))\n",
    "print(np.shape(training_un_lov))\n",
    "print(np.shape(testing_un_lov))\n",
    "print(np.shape(training_disp_ray))\n",
    "print(np.shape(testing_disp_ray))\n",
    "print(np.shape(training_un_ray))\n",
    "print(np.shape(testing_un_ray))\n",
    "print(np.shape(training_svel))\n",
    "print(np.shape(testing_svel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model  \n",
    "Number of epochs and batch size can be adapted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train model\n",
    "results=model.fit([training_disp_lov,training_un_lov,training_disp_ray,training_un_ray], [training_svel,training_depth],batch_size=128, epochs=100,verbose=1, shuffle=True,validation_split=0.1,callbacks=[NAN])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save('./model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model\n",
    "loss_and_metrics = model.evaluate([testing_disp_lov,testing_un_lov,testing_disp_ray,testing_un_ray], [testing_svel,testing_depth])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Curves\n",
    "fig=plt.figure(figsize=[8,6])\n",
    "\n",
    "ax=fig.add_subplot(111)\n",
    "ax.plot(results.history['loss'],'r',linewidth=3.0)\n",
    "ax.plot(results.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance  \n",
    "Use previously unseen input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test trained model on unseen data; insert your data here\n",
    "unseen_disp_lov=np.load(\"love_disp_unseen.npy\")\n",
    "unseen_un_lov=np.load(\"love_un_unseen.npy\")\n",
    "unseen_disp_ray=np.load(\"ray_disp_unseen.npy\")\n",
    "unseen_un_ray=np.load(\"ray_un_unseen.npy\")\n",
    "unseen_vel=np.load(\"vel_unseen.npy\")\n",
    "unseen_depth=np.load(\"depth_unseen.npy\")\n",
    "\n",
    "print(np.shape(unseen_disp_lov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data\n",
    "unseen_disp_lov_scal=scaler1.transform(unseen_disp_lov)\n",
    "unseen_un_lov_scal=scaler2.transform(unseen_un_lov)\n",
    "unseen_disp_ray_scal=scaler3.transform(unseen_disp_ray)\n",
    "unseen_un_ray_scal=scaler4.transform(unseen_un_ray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict velocity structure\n",
    "predictions1,predictions2 = model.predict([unseen_disp_lov_scal,unseen_un_lov_scal,unseen_disp_ray_scal,unseen_un_ray_scal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((predictions1.shape)) #30 values for means, 10 for stdevs and 10 for alphas (depends on number of kernels)\n",
    "print((predictions2.shape)) #depth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort output for plotting of results\n",
    "\n",
    "vel=np.linspace(0,2.5,6000) #samples for x-axis\n",
    "\n",
    "errors=[]\n",
    "predicted_means=[]\n",
    "kernels=[]\n",
    "\n",
    "for n in range(len(predictions1)):\n",
    "    means=predictions1[n][0:y_size*m] #split prediction vector in means, stdev and alpha\n",
    "    stdev=predictions1[n][y_size*m:y_size*m+m]\n",
    "    alpha=predictions1[n][y_size*m+m:]\n",
    "    model_mean=[]\n",
    "    model_kernel= []\n",
    "    for o in range(y_size):\n",
    "        add=0\n",
    "        for i in range(m):  #add all gaussian kernels\n",
    "            kernel=alpha[i]*stats.norm.pdf(vel,means[o*m+i],abs(stdev[i]))\n",
    "            add = add + kernel\n",
    "        vel_estimate=vel[np.argmax(add)] #take maximum in gaussian kernel as mean velocity estimate for the layer\n",
    "        model_mean.append(vel_estimate)\n",
    "        model_kernel.append(add)\n",
    "    predicted_means.append(model_mean)\n",
    "    kernels.append(model_kernel)\n",
    "    \n",
    "    #compute error between predicted velocity value and actual value\n",
    "    diff=[]\n",
    "    for o in range(y_size):\n",
    "        d=(model_mean[o]-unseen_vel[n][o]) \n",
    "        diff.append(d)\n",
    "    errors.append(diff)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#depth errors\n",
    "depth_error=[]\n",
    "for n in range(len(predictions2)):\n",
    "    layer_err=[]\n",
    "    for i in range(len(predictions2[n])):\n",
    "        err=predictions2[n][i]-unseen_depth[n][i]\n",
    "        layer_err.append(err)\n",
    "    depth_error.append(layer_err)\n",
    "\n",
    "print(np.shape(depth_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kernels(prof):\n",
    "    '''plot kernels for each layer\n",
    "        \n",
    "        prof = number of profile\n",
    "    '''\n",
    "    \n",
    "    vel=np.linspace(0,2.5,6000)\n",
    "    fig= plt.figure(figsize=(15,15))\n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "\n",
    "    num=1\n",
    "\n",
    "    for i in range(y_size):\n",
    "        ax=fig.add_subplot(4,y_size,num)\n",
    "        ax.plot(vel,kernels[prof][i],color=\"black\")\n",
    "        x = ax.lines[0].get_xdata()\n",
    "        y = ax.lines[0].get_ydata()\n",
    "        plt.axvline(unseen_vel[prof][i],color='red',linestyle=\"--\") #actual velocity within layer\n",
    "        plt.xlabel(\"Vs (km/s)\")\n",
    "        plt.title(\"layer{}\".format(num))\n",
    "        num=num+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kernels(990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_profile(prof):\n",
    "    '''plot profile with uncertainties\n",
    "    \n",
    "        prof = number of model\n",
    "    '''\n",
    "    \n",
    "    x=np.linspace(0,2.5,6000) #samples for x-axis\n",
    "    y=np.linspace(0,100,100) #samples for y-axis\n",
    "    \n",
    "    #transform layer depth into layer thickness\n",
    "    #predicted layer depth\n",
    "    layer_thickness_pred=[]\n",
    "    lay1=int(predictions2[prof][0])\n",
    "    layer_thickness_pred.append(lay1)\n",
    "\n",
    "    for n in range(y_size-2):\n",
    "        lay=int(predictions2[prof][n+1])-np.sum(layer_thickness_pred)\n",
    "        layer_thickness_pred.append(lay)\n",
    "    \n",
    "    lay_last=100-np.sum(layer_thickness_pred)\n",
    "    layer_thickness_pred.append(lay_last)\n",
    "    print(\"predicted layer thickness:\",layer_thickness_pred)\n",
    "    \n",
    "\n",
    "    #true layer depth\n",
    "    layer_thickness_true=[]\n",
    "    lay1_t=int(unseen_depth[prof][0])\n",
    "    layer_thickness_true.append(lay1_t)\n",
    "\n",
    "    for n in range(y_size-2):\n",
    "        lay_t=int(unseen_depth[prof][n+1])-np.sum(layer_thickness_true)\n",
    "        layer_thickness_true.append(lay_t)\n",
    "    \n",
    "    lay_last_t=100-np.sum(layer_thickness_true)\n",
    "    layer_thickness_true.append(lay_last_t)\n",
    "    print(\"true layer thickness:\",layer_thickness_true)\n",
    "    \n",
    "    \n",
    "    #mean velocity structure\n",
    "    true_model=[]\n",
    "    for n in range(y_size):\n",
    "        for i in range(layer_thickness_true[n]):\n",
    "            true_model.append(unseen_vel[prof][n])\n",
    "        \n",
    "    pred_model=[]\n",
    "    for n in range(y_size):\n",
    "        for i in range(layer_thickness_pred[n]):\n",
    "            pred_model.append(predicted_means[prof][n])\n",
    "            \n",
    "    \n",
    "    #probability distribution\n",
    "    extent = [x.min(), x.max(), y.min(), y.max()]\n",
    "    layers=np.flip(layer_thickness_pred) #layers have to be in reverse order!!\n",
    "\n",
    "    z_list=[]\n",
    "    for n in range(y_size):\n",
    "        for i in range(layers[n]):\n",
    "            pdf = kernels[prof][y_size-1-n]\n",
    "            z_list.append(pdf)\n",
    "            \n",
    "    \n",
    "    #plot figure\n",
    "    fig=plt.figure(figsize=(4,7))\n",
    "\n",
    "    ax1=fig.add_subplot(111)\n",
    "    ax1.plot(true_model,y,label=\"True model\")\n",
    "    ax1.plot(pred_model,y,label=\"ML model\")\n",
    "    ax1.imshow(z_list, extent=extent, aspect = 'auto',cmap=plt.cm.Greys)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xticks(size=14)\n",
    "    plt.yticks(size=14)\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.xlim(0,2.5)\n",
    "    plt.xlabel(\"Vs (km/s)\",fontsize=14)\n",
    "    plt.ylabel(\"Depth (m)\",fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_profile(990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot errors velocity\n",
    "fig=plt.figure(figsize=(6,12))\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "\n",
    "num=1\n",
    "for n in range(y_size):\n",
    "    ax=fig.add_subplot(y_size,1,n+1)\n",
    "    e=[]\n",
    "    for i in range(len(errors)):\n",
    "        e.append(errors[i][n])\n",
    "    ax.hist(e, bins=100)\n",
    "    plt.title(\"layer{}\".format(num))\n",
    "    plt.ylabel(\"number of models\")\n",
    "    plt.xlabel(\"velolcity difference (km/s)\")\n",
    "    num=num+1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot errors depth\n",
    "fig=plt.figure(figsize=(6,12))\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "\n",
    "num=1\n",
    "for n in range(y_size):\n",
    "    ax=fig.add_subplot(y_size,1,n+1)\n",
    "    e=[]\n",
    "    for i in range(len(depth_error)):\n",
    "        e.append(depth_error[i][n])\n",
    "    ax.hist(e, bins=100)\n",
    "    plt.title(\"layer{}\".format(num))\n",
    "    plt.ylabel(\"number of models\")\n",
    "    plt.xlabel(\"depth difference (m)\")\n",
    "    num=num+1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot pearson correlation to visualize error for velocity estimate\n",
    "\n",
    "line=np.linspace(0,2.5,500)\n",
    "\n",
    "fig=plt.figure(figsize=(4,10))\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "num=1\n",
    "for n in range(y_size):\n",
    "    ax=fig.add_subplot(y_size,1,n+1)\n",
    "    x=[]\n",
    "    for i in range(len(predicted_means)):\n",
    "        x.append(predicted_means[i][n])\n",
    "    y=[]\n",
    "    for i in range(len(unseen_vel)):\n",
    "        y.append(unseen_vel[i][n])\n",
    "        \n",
    "    corr =stats.pearsonr(x, y)\n",
    "    Z, xax,yax=np.histogram2d(x,y,bins=50,range=[[0,2.5],[0,2.5]])\n",
    "    \n",
    "    ax.pcolormesh(xax, yax, Z.T, cmap=\"Greys\")\n",
    "    plt.plot(line,line, color=\"red\")\n",
    "    plt.xlabel(\"Predicted velocity (km/s)\",fontsize=12)\n",
    "    plt.ylabel(\"True velocity (km/s)\",fontsize=12)\n",
    "    plt.title(\"layer {} - corr {}\".format(num,\"%0.3f\" % (corr[0])))\n",
    "    num=num+1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot pearson correlation to visualize error for depth estimate\n",
    "\n",
    "line=np.linspace(0,100,100)\n",
    "\n",
    "fig=plt.figure(figsize=(4,10))\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "num=1\n",
    "for n in range(y_size):\n",
    "    ax=fig.add_subplot(y_size,1,n+1)\n",
    "    x=[]\n",
    "    for i in range(len(predictions2)):\n",
    "        x.append(predictions2[i][n])\n",
    "    y=[]\n",
    "    for i in range(len(unseen_depth)):\n",
    "        y.append(unseen_depth[i][n])\n",
    "        \n",
    "    corr =stats.pearsonr(x, y)\n",
    "    Z, xax,yax=np.histogram2d(x,y,bins=100,range=[[0,100],[0,100]])\n",
    "    \n",
    "    ax.pcolormesh(xax, yax, Z.T, cmap=\"Greys\")\n",
    "    plt.plot(line,line, color=\"red\")\n",
    "    plt.xlabel(\"Predicted thickness (m)\",fontsize=12)\n",
    "    plt.ylabel(\"True thickness (m)\",fontsize=12)\n",
    "    plt.title(\"layer {} - corr {}\".format(num,\"%0.3f\" % (corr[0])))\n",
    "    num=num+1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "Earp, S., Curtis, A., Zhang, X., & Hansteen, F. (2020). Probabilistic neural network tomography across Grane field (North Sea) from surface wave dispersion data. Geophysical Journal International, 223(3), 1741-1757."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
